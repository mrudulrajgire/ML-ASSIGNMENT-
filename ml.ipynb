{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzZqf6vrbtMr",
        "outputId": "d5e3799b-93ec-4a3e-cb7c-0490ddbecb1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'garbage-classification' dataset.\n",
            "Path to dataset files: /kaggle/input/garbage-classification\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mostafaabla/garbage-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2a2122a",
        "outputId": "1f52c899-9012-4e07-adc9-bd99106d5536"
      },
      "source": [
        "# List all files and directories in the dataset path\n",
        "items_in_path = os.listdir(path)\n",
        "\n",
        "# Iterate through the items and print the names of directories\n",
        "print(\"Directories within the dataset path:\")\n",
        "for item in items_in_path:\n",
        "    item_path = os.path.join(path, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        print(item)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories within the dataset path:\n",
            "garbage_classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50d9bc06",
        "outputId": "dbb5c056-43e5-485a-8961-4ee39e64d5b5"
      },
      "source": [
        "import os\n",
        "\n",
        "# List all files and directories in the dataset path\n",
        "items_in_path = os.listdir(path)\n",
        "\n",
        "# Iterate through the items and print the names of directories\n",
        "print(\"Directories within the dataset path:\")\n",
        "for item in items_in_path:\n",
        "    item_path = os.path.join(path, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        print(item)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories within the dataset path:\n",
            "garbage_classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU84QjrVcbN-",
        "outputId": "6f2aca9a-fcc3-4b45-df67-0468db4d2215"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the path to the directory containing the image data\n",
        "image_dir = os.path.join(path, 'garbage_classification')\n",
        "\n",
        "# Load the images from the directory\n",
        "image_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    image_dir,\n",
        "    labels='inferred',  # Corrected: Infer labels from directory names\n",
        "    label_mode='categorical', # Set label_mode to categorical\n",
        "    image_size=(128, 128),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1\n",
        "image_dataset = image_dataset.map(lambda x, y: (x / 255.0, y))\n",
        "\n",
        "print(\"Image dataset loaded and preprocessed successfully.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15515 files belonging to 12 classes.\n",
            "Image dataset loaded and preprocessed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ad4f75"
      },
      "source": [
        "**Reasoning**:\n",
        "Determine the total number of batches, calculate the number of batches for each split, and then create the splits using take and skip. Cache and prefetch the datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b6a9bda",
        "outputId": "ea3398aa-e61c-4966-fd64-20e08e3e8689"
      },
      "source": [
        "# Determine the total number of batches\n",
        "total_batches = tf.data.experimental.cardinality(image_dataset).numpy()\n",
        "print(f\"Total number of batches: {total_batches}\")\n",
        "\n",
        "# Define the split ratios\n",
        "train_split = 0.8\n",
        "val_split = 0.1\n",
        "test_split = 0.1\n",
        "\n",
        "# Calculate the number of batches for each split\n",
        "train_batches = int(total_batches * train_split)\n",
        "val_batches = int(total_batches * val_split)\n",
        "test_batches = total_batches - train_batches - val_batches # Ensure all batches are included\n",
        "\n",
        "print(f\"Number of training batches: {train_batches}\")\n",
        "print(f\"Number of validation batches: {val_batches}\")\n",
        "print(f\"Number of test batches: {test_batches}\")\n",
        "\n",
        "# Create the training, validation, and test datasets\n",
        "train_dataset = image_dataset.take(train_batches)\n",
        "val_dataset = image_dataset.skip(train_batches).take(val_batches)\n",
        "test_dataset = image_dataset.skip(train_batches + val_batches)\n",
        "\n",
        "# Cache and prefetch the datasets\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"Dataset split into training, validation, and test sets.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of batches: 485\n",
            "Number of training batches: 388\n",
            "Number of validation batches: 48\n",
            "Number of test batches: 49\n",
            "Dataset split into training, validation, and test sets.\n"
          ]
        }
      ]
    }
  ]
}